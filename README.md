# Сервис Анализа Отзывов

Этот проект представляет собой ML-сервис для анализа отзывов граждан. Он использует LLM (Large Language Model) для классификации отзывов по категориям (ЖКХ, Транспорт, Здравоохранение и др.), определения тональности и извлечения конструктивных идей для улучшения сервисов.

## Функциональность

*   **Классификация**: Определение одной или нескольких категорий для каждого отзыва.
*   **Анализ тональности**: Определение тональности (положительно, отрицательно, нейтрально) для каждой категории и отзыва в целом.
*   **Извлечение идей**: Формирование списка предложений по улучшению на основе негативных или нейтральных отзывов.

## Структура Проекта

Весь исходный код находится в директории `src/`:

*   `src/agent/`: Логика агента на базе LangGraph (промпты, граф состояний, утилиты).
*   `src/services/`: Бизнес-логика, включая `PredictionService` для обработки батчей отзывов.
*   `src/endpoints/`: API эндпоинты (FastAPI).
*   `src/settings.py`: Конфигурация приложения.

Тесты находятся в директории `tests/`.

## Установка и Настройка

1.  **Клонирование репозитория:**
    ```bash
    git clone <repository_url>
    cd <project_directory>
    ```

2.  **Создание виртуального окружения:**
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\activate
    # Linux/MacOS
    source .venv/bin/activate
    ```

3.  **Установка зависимостей:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Настройка окружения:**
    Создайте файл `.env` в корне проекта и добавьте ваш ключ API (OpenRouter/OpenAI):
    ```env
    OPENROUTER_API_KEY=your_api_key_here
    LLM_NAME=qwen/qwen3-235b-a22b:free
    ```

## Использование

### 1. Запуск через командную строку (CLI)

Вы можете обработать файл с отзывами (JSON) локально:

```bash
python main.py reviews.json
```
*   `reviews.json` должен содержать список строк или объектов `{"id": 1, "text": "..."}`.
*   Добавьте флаг `--few-shot` для использования few-shot промптов.

### 2. Запуск API сервера

Запустите сервер FastAPI:

```bash
uvicorn app:app --reload
```

Документация API будет доступна по адресу: `http://127.0.0.1:8000/docs`.

**Пример запроса к API:**
`POST /api/v1/predict`
```json
{
  "reviews": [
    {
      "id": 1,
      "text": "Автобусы ходят редко, приходится ждать по 40 минут."
    }
  ],
  "use_few_shot": false
}
```

### 3. Запуск через Docker

Проект поддерживает запуск в Docker контейнерах. Это удобно для развертывания или локальной разработки в изолированной среде.

1.  **Сборка и запуск:**
    ```bash
    docker-compose up --build
    ```
    Это запустит сервис на порту `8000`.

2.  **Конфигурация (Важно):**
    *   По умолчанию `docker-compose.yaml` пытается запустить локальный LLM сервер (`llm-cpu-server`).
    *   Если вы используете **OpenRouter**, вы можете запустить только сервис приложения:
        ```bash
        docker-compose up --build sentiment-service
        ```
    *   Если вы хотите использовать **локальную модель**:
        1.  Положите GGUF модель в папку `models/`.
        2.  В `.env` укажите имя файла модели в `LLM_NAME` (например, `LLM_NAME=llama-3-8b.Q4_K_M.gguf`).
        3.  Убедитесь, что `BASE_URL` указывает на локальный контейнер (обычно это не требуется менять, если сервис ходит напрямую, но для `llm-cpu-server` адрес внутри сети docker будет `http://llm_cpu_server:8080/v1`).

### 4. Запуск экспериментов

Для оценки качества модели используется скрипт `experiment.py`, который логирует метрики в MLflow.

```bash
python experiment.py \
  --service-url http://localhost:8000/api/v1/predict \
  --mlflow-url http://localhost:5000 \
  --model-name "qwen-2.5" \
  --csv-path data/reviews.csv
```

## Тестирование

Для запуска тестов используйте `pytest`:

```bash
python -m pytest tests/
```
